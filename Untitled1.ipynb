{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the command to get the embeddings of the upcoming weeks. We also note this model (to get the embeddings as well as  one first model to learn the embeddings). This model will then be used to genereate embeddings for all upcoming weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment starts:  1627093784.306\n",
      "Namespace(act='relu', ada_lr=0.8, agg='sum', alpha=10, batch_size=128, closs='bce', data='real-t', device='0', devices=['0', '1', '2', '3'], dim=16, epoch=5, final_inspection_rate=5.0, fusion='concat', head_num=4, identifier='1627093784.306', initial_inspection_rate=20.0, inspection_plan='direct_decay', l2=0.01, lr=0.005, mode='scratch', numweeks=1, output='result-1627093784.306', rev_func='log', rloss='full', sampling='hybrid', save=0, semi_supervised=0, ssl_strategy='random', subsamplings='bATE/DATE', test_from='20160701', test_length=350, train_from='20160101', uncertainty='naive', use_self=1, valid_length=30, weights='0/1')\n",
      "Before masking:\n",
      " 0    373394\n",
      "1     28939\n",
      "Name: illicit, dtype: int64\n",
      "NumExpr defaulting to 6 threads.\n",
      "After masking:\n",
      " 0.0    74643\n",
      "1.0     5824\n",
      "Name: illicit, dtype: int64\n",
      "Inspection rate for testing periods: [5.]\n",
      "Data size:\n",
      "Train labeled: (80467, 41), Train unlabeled: (321866, 41), Valid labeled: (69932, 41), Valid unlabeled: (0, 13), Test: (857886, 41)\n",
      "Checking label distribution\n",
      "Training: 0.0780247310531463\n",
      "Validation: 0.09654253234025872\n",
      "Testing: 0.08409417455730098\n",
      "Test episode: #0, Current inspection rate: 5.0\n",
      "(80467, 41), (857886, 41)\n",
      "<Hybrid> Querying 42894 (=100.0%) items using the <query_strategies.DATE.DATESampling object at 0x2b997e445898> subsampler\n",
      "Training XGBoost model...\n",
      "[11:31:02] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Mode: scratch, Episode: 0\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "\n",
      "Training DATE model ...\n",
      "------------\n",
      "Validate at epoch 1\n",
      "CLS loss: 0.3298, REG loss: 0.0257\n",
      "Checking top 5% suspicious transactions: 3497\n",
      "Precision: 0.5619, Recall: 0.3191, Revenue: 0.4094\n",
      "Overall F1:0.4202, AUC:0.6869, F1-top:0.4856\n",
      "/home/intern/kien/Customs-Fraud-Detection/intermediary/saved_models/DATE-1627093784.306.pkl\n",
      "------------\n",
      "Validate at epoch 2\n",
      "CLS loss: 0.3531, REG loss: 0.0257\n",
      "Checking top 5% suspicious transactions: 3497\n",
      "Precision: 0.5633, Recall: 0.3200, Revenue: 0.4181\n",
      "Overall F1:0.4261, AUC:0.7444, F1-top:0.4907\n",
      "/home/intern/kien/Customs-Fraud-Detection/intermediary/saved_models/DATE-1627093784.306.pkl\n",
      "------------\n",
      "Validate at epoch 3\n",
      "CLS loss: 0.4252, REG loss: 0.0269\n",
      "Checking top 5% suspicious transactions: 3497\n",
      "Precision: 0.5342, Recall: 0.3034, Revenue: 0.3893\n",
      "Overall F1:0.3978, AUC:0.7231, F1-top:0.4617\n",
      "------------\n",
      "Validate at epoch 4\n",
      "CLS loss: 0.4563, REG loss: 0.0271\n",
      "Checking top 5% suspicious transactions: 3497\n",
      "Precision: 0.5325, Recall: 0.3024, Revenue: 0.3894\n",
      "Overall F1:0.3976, AUC:0.7254, F1-top:0.4609\n",
      "------------\n",
      "Validate at epoch 5\n",
      "CLS loss: 0.4940, REG loss: 0.0269\n",
      "Checking top 5% suspicious transactions: 3497\n",
      "Precision: 0.5093, Recall: 0.2893, Revenue: 0.3716\n",
      "Overall F1:0.3684, AUC:0.7130, F1-top:0.4404\n",
      "Early stopping...\n",
      "\n",
      "--------Evaluating DATE model---------\n",
      "CLS loss: 0.3531, REG loss: 0.0257\n",
      "CLS loss: 0.3365, REG loss: 0.0250\n",
      "Checking top 5% suspicious transactions: 42895\n",
      "Precision: 0.4363, Recall: 0.2812, Revenue: 0.3427\n",
      "CLS loss: 0.3365, REG loss: 0.0250\n",
      "Training XGBoost model...\n",
      "[11:45:56] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Mode: scratch, Episode: 0\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "\n",
      "Training DATE model ...\n",
      "------------\n",
      "Validate at epoch 1\n",
      "CLS loss: 0.3001, REG loss: 0.0256\n",
      "Checking top 5% suspicious transactions: 3497\n",
      "Precision: 0.5765, Recall: 0.3274, Revenue: 0.4319\n",
      "Overall F1:0.4448, AUC:0.7602, F1-top:0.5042\n",
      "/home/intern/kien/Customs-Fraud-Detection/intermediary/saved_models/DATE-1627093784.306.pkl\n",
      "------------\n",
      "Validate at epoch 2\n",
      "CLS loss: 0.3344, REG loss: 0.0257\n",
      "Checking top 5% suspicious transactions: 3497\n",
      "Precision: 0.5573, Recall: 0.3166, Revenue: 0.4189\n",
      "Overall F1:0.4268, AUC:0.7399, F1-top:0.4881\n",
      "------------\n",
      "Validate at epoch 3\n",
      "CLS loss: 0.3973, REG loss: 0.0266\n",
      "Checking top 5% suspicious transactions: 3497\n",
      "Precision: 0.5479, Recall: 0.3112, Revenue: 0.4058\n",
      "Overall F1:0.4105, AUC:0.7376, F1-top:0.4768\n",
      "------------\n",
      "Validate at epoch 4\n",
      "CLS loss: 0.4274, REG loss: 0.0279\n",
      "Checking top 5% suspicious transactions: 3497\n",
      "Precision: 0.5413, Recall: 0.3075, Revenue: 0.4013\n",
      "Overall F1:0.3987, AUC:0.7311, F1-top:0.4713\n",
      "Early stopping...\n",
      "\n",
      "--------Evaluating DATE model---------\n",
      "CLS loss: 0.3001, REG loss: 0.0256\n",
      "CLS loss: 0.2888, REG loss: 0.0255\n",
      "Checking top 5% suspicious transactions: 42895\n",
      "Precision: 0.4447, Recall: 0.2867, Revenue: 0.3504\n",
      "CLS loss: 0.2888, REG loss: 0.0255\n",
      "# of unique queried item: 42894, # of queried item: 42894, # of samples to be queried: 42894\n",
      "--------Evaluating the model---------\n",
      "Precision: 0.4363, Recall: 0.2812, Revenue: 0.3427\n",
      "Metrics Active DATE:\n",
      " Pr@5.0:0.4363, Re@5.0:0.2812 Rev@5.0:0.3427\n",
      "Simulation period is over.\n",
      "Terminating ...\n"
     ]
    }
   ],
   "source": [
    "day = 20160701\n",
    "! export CUDA_VISIBLE_DEVICES=3 && python main.py --data real-t --semi_supervised 0 --batch_size 128 --sampling hybrid --subsamplings bATE/DATE --weights 0/1 --mode scratch --train_from 20160101 --test_from $day --test_length 350 --valid_length 30 --initial_inspection_rate 20 --final_inspection_rate 5 --epoch 5 --closs bce --rloss full --save 0 --numweeks 1 --inspection_plan direct_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will store the embeddings in the directory './intermediary/embeddings/embd_0.pickle'\n",
    "These embeddings will be domain-shift calculated between weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Domain shifts between weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "import ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./intermediary/embeddings/embd_0.pickle\",\"rb\") as f :\n",
    "    processed_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching the datapoints together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datestart = '16-07-01'\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "date = datetime.strptime(datestart, \"%y-%m-%d\")\n",
    "\n",
    "datelist0 = []\n",
    "\n",
    "for i in range(51):\n",
    "    \n",
    "    delta = i * 7\n",
    "    \n",
    "    new_date = date + timedelta(days=delta)\n",
    "    \n",
    "    datelist0.append(new_date.strftime('%y-%m-%d'))\n",
    "    \n",
    "enddate = datelist0[-1]\n",
    "\n",
    "data = pd.read_csv('tdata.csv')\n",
    "data = data[(data['sgd.date'] >= datestart) & (data['sgd.date'] < enddate)]\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat weekly data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = []\n",
    "\n",
    "for i in range(len(datelist0) - 1):\n",
    "    \n",
    "    data0 = data[(data['sgd.date'] >= datelist0[i]) & (data['sgd.date'] < datelist0[i+1])]\n",
    "    \n",
    "    start = data0.index[0]\n",
    "    end = data0.index[-1]\n",
    "    \n",
    "    xd = []\n",
    "    \n",
    "    for j in range(start, end + 1):\n",
    "        xd.append(processed_data[datelist[j]].reshape(1, -1))\n",
    "        \n",
    "    full_data.append(torch.cat(xd, axis=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_shift(xs, xt):\n",
    "    \n",
    "    M = torch.cdist(xs, xt)\n",
    "    \n",
    "    lol = torch.max(M)\n",
    "\n",
    "    M = M/lol\n",
    "    \n",
    "    M = M.data.cpu().numpy()\n",
    "    \n",
    "    a = [1/xs.shape[0]] * xs.shape[0]\n",
    "    b = [1/xt.shape[0]] * xt.shape[0]\n",
    "    \n",
    "    prep = ot.emd2(a, b, M)\n",
    "\n",
    "    return prep * lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arraycom = []\n",
    "for i in range(81, len(full_data) - 1):\n",
    "    \n",
    "    stack = []\n",
    "    \n",
    "    for j in range(60):\n",
    "        \n",
    "        indices = torch.tensor(random.sample(range(full_data[i].shape[0]), 500))\n",
    "        indices2 = torch.tensor(random.sample(range(full_data[i+1].shape[0]), 500))\n",
    "        \n",
    "        xs = full_data[i][indices]\n",
    "        xt = full_data[i+1][indices2]\n",
    "        \n",
    "        print(domain_shift(xs, xt))\n",
    "        \n",
    "        stack.append(domain_shift(xs, xt))\n",
    "        \n",
    "    xd = np.mean(stack)\n",
    "    \n",
    "    print(xd)\n",
    "    \n",
    "    arraycom.append(xd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arraycom will contain domainshifts information  through 100 weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is about extracting the best probability of resampling through each week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction(lol):\n",
    "    for i in lol:\n",
    "        if 'Pr@5.0' in i:\n",
    "            xd = i.split(', ')[0]\n",
    "            value = float(xd.split(':')[1])\n",
    "            \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = []\n",
    "for i in range(0, len(datelist0) - 2):\n",
    "    \n",
    "    highest = 0\n",
    "    day = datelist0[i]\n",
    "    \n",
    "    for j in range(1, 10):\n",
    "        \n",
    "        ratio = j * 0.1\n",
    "        bratio =  1 - ratio\n",
    "        \n",
    "        lol = ! export CUDA_VISIBLE_DEVICES=3 && python main.py --data real-t --semi_supervised 0 --batch_size 128 --sampling hybrid --subsamplings xgb/random --weights $ratio/$bratio --mode scratch --train_from 20160101 --test_from $day --test_length 7 --valid_length 30 --initial_inspection_rate 20 --final_inspection_rate 5 --epoch 5 --closs bce --rloss full --save 0 --numweeks 2 --inspection_plan direct_decay\n",
    "        \n",
    "        value = extract(lol)\n",
    "        if value > highest:\n",
    "            smt = bratio\n",
    "            \n",
    "    probs.append(smt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probs should contain the best resampling ratio. Now we come to find the coefficients. First transform the probs result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = []\n",
    "for i in probs:\n",
    "    news.append(np.log(i/(1-i)))\n",
    "\n",
    "newcom = np.reshape(arraycom, (-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(newcom, news)\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
