import numpy as np
import random
import sys
import math
import ot
from datetime import datetime, timedelta
import pickle
import numpy as np
import pandas as pd
import torch
from .strategy import Strategy
from .DATE import DATESampling
from .hybrid import HybridSampling

            
class POTSampling(HybridSampling):
    """ Optimal Transport strategy: Using POT library to measure domain shift and control subsampler weights 
        Reference: https://pythonot.github.io/all.html?highlight=emd2#ot.emd2 """


    def __init__(self, args):
        super(POTSampling,self).__init__(args)
        assert len(self.subsamps) == 2
        
        self.intercept = -1.88325971450706   # SD: How to decide this value?
        self.coef = 0.00709741           # SD: How to decide this value? (Very small)

        # self.data already exists - In main.py, we declared in: sampler.set_data(data)
        

    def small_shift(self, xs, xt):
        # This code should be optimized (tensor calculation, instead of converting it to numpy and use for loop)
        
        M = np.zeros(shape = (xs.shape[0], xt.shape[0]))
        for i in range(xs.shape[0]):
            for j in range(xt.shape[0]):
                M[i][j] = np.square(torch.sum((xs[i] - xt[j]) ** 2).item())
        lol = np.max(M)
        M = M/np.max(M)
        a = [1/xs.shape[0]] * xs.shape[0]
        b = [1/xt.shape[0]] * xt.shape[0]
        prep = ot.emd2(a, b, M)
        return prep * lol


    # def concat(self, data0):
        
    #     # REMOVED (Does not needed anymore)
    #     # For online setting: Embeddings up to current data points should be calculated on-the-fly - Assumption: For the current week, the system cannot observe the future data points. 
    #     # Below pickle file is generated by running below commands
    #     # export CUDA_VISIBLE_DEVICES=3 && python main.py --data real-t --semi_supervised 0 --batch_size 128 --sampling hybrid --subsamplings bATE/DATE --weights 0/1 --mode scratch --train_from 20160101 --test_from 20160701 --test_length 700 --valid_length 30 --initial_inspection_rate 20 --final_inspection_rate 5 --epoch 5 --closs bce --rloss full --save 0 --numweeks 1 --inspection_plan direct_decay
      
    #     with open("./data/emb_0_3.pickle","rb") as f :
    #         processed_data = pickle.load(f)

    #     xd = []
        
    #     datelist = list(processed_data.keys())
        
    #     for j in data0.index:
    #         xd.append(processed_data[datelist[j]].reshape(1, -1))

    #     array1 = torch.cat(xd, axis=0)
    #     return array1

    
    def generate_DATE_embeddings(self):
        # For every week, we train DATE model and get embeddings for validation set and test set.
        # We already implemented a method 'get_embedding' in our previous code, thus not used Tosha's embedding.
        # Since this code run DATE, the terminal becomes dirty because its verbosity. You can consider changing print logging option in DATE.py.
        # When DATE for POT is run, verbosity = 0, otherwise verbosity = 1 

        date_sampler = DATESampling(self.args)
        date_sampler.set_data(self.data)
        date_sampler.train_xgb_model()
        date_sampler.prepare_DATE_input()
        date_sampler.train_DATE_model()
        valid_embeddings = torch.stack(date_sampler.get_embedding_valid())  # Embeddings for validation data
        test_embeddings = torch.stack(date_sampler.get_embedding_test())         # Embeddings for test data

        return valid_embeddings, test_embeddings


    def domain_shift(self):
        # Measure domain shift between validation data and test data.
    
        valid_embeddings, test_embeddings = self.generate_DATE_embeddings()

        stack = []
        
        # The code becomes slow when we control this number larger, need to optimize the calculation in 'small_shift'
        for j in range(5):                  # 60
            ind_valid = torch.tensor(random.sample(range(len(valid_embeddings)), 50)).cuda()       # 500
            ind_test = torch.tensor(random.sample(range(len(test_embeddings)), 50)).cuda()         # 500

            xv = torch.index_select(valid_embeddings, 0, ind_valid)
            xt = torch.index_select(test_embeddings, 0, ind_test)

            stack.append(self.small_shift(xv, xt))
        
        xd = np.mean(stack)
        return xd


    def update_subsampler_weights(self):  
        
        domshift = self.domain_shift()

        # Since the self.coef is very small (0.007), large domain shift leads to smaller weight - exploration ratio. Is it what you intended? 
        weight = np.exp(self.intercept + self.coef * domshift)/ (1 + np.exp(self.intercept + self.coef * domshift))
        
        self.weight = round(weight, 2).item()
        print(type(self.weight))
        
        print(self.weight)

        self.weights = [1 - self.weight, self.weight]
        
        #self.weights = [1 - 0.14, 0.14]





    